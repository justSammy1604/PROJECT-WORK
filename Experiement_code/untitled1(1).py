# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14xLL7GtneZ4UwucZ96s46dcl8OlxSZr9
"""

!pip install datasets jsonlines bert-score langchain-cohere
!pip install chromadb
!pip install langchain-google-genai

# Create the directory if it doesn't exist
!mkdir -p ./datasets/squad/
# Download the dataset from Kaggle
!curl -L -o ./datasets/squad/stanford-question-answering-dataset.zip \
    https://www.kaggle.com/api/v1/datasets/download/stanfordu/stanford-question-answering-dataset

# Unzip the dataset into the correct directory
!unzip ./datasets/squad/stanford-question-answering-dataset.zip -d ./datasets/squad

# Remove the ZIP file after extraction
!rm ./datasets/squad/stanford-question-answering-dataset.zip

# Create the directory if it doesn't exist
!mkdir -p ./datasets/hotpotqa

# Download the dataset from Kaggle
!curl -L -o ./datasets/hotpotqa/hotpotqa-question-answering-dataset.zip \
    https://www.kaggle.com/api/v1/datasets/download/jeromeblanchet/hotpotqa-question-answering-dataset

# Unzip the dataset into the correct directory
!unzip ./datasets/hotpotqa/hotpotqa-question-answering-dataset.zip -d ./datasets/hotpotqa

# Remove the ZIP file after extraction
!rm ./datasets/hotpotqa/hotpotqa-question-answering-dataset.zip

import json

def load_squad(filename):
    with open(filename, "r") as f:
        squad_data = json.load(f)

    processed_data = []
    for article in squad_data["data"]:
        for paragraph in article["paragraphs"]:
            context = paragraph["context"]
            for qa in paragraph["qas"]:
                question = qa["question"]
                answer = qa["answers"][0]["text"] if qa["answers"] else ""

                processed_data.append({
                    "question": question,
                    "answer": answer,
                    "context": context
                })

    return processed_data

# Load preprocessed SQuAD datasets
# squad_small = load_squad("/content/datasets/squad/dev-v1.1.json")
squad_large = load_squad("/content/datasets/squad/train-v1.1.json")

print(squad_large[0])

''' import json
def load_hotpotqa(filename):
    with open(filename, "r") as f:
        hotpot_data = json.load(f)

    processed_data = []
    for item in hotpot_data:
        question = item["question"]
        answer = item["answer"]

        # Flatten supporting facts
        context = " ".join([para_text for para_title, para_texts in item["context"] for para_text in para_texts])

        processed_data.append({
            "question": question,
            "answer": answer,
            "context": context
        })

    return processed_data

# Load preprocessed HotpotQA datasets
hotpotqa_small = load_hotpotqa("/content/datasets/hotpotqa/hotpot_dev_distractor_v1.json")
# hotpotqa_medium = load_hotpotqa("/content/datasets/hotpotqa/hotpot_test_fullwiki_v1.json")
# hotpotqa_large = load_hotpotqa("/content/datasets/hotpotqa/hotpot_train_v1.1.json")
 '''

from langchain.vectorstores import Chroma
# from langchain.embeddings.openai import OpenAIEmbeddings
from langchain_cohere import CohereEmbeddings
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain.schema import Document
import time

# embedding_model = CohereEmbeddings(model="embed-multilingual-v2.0", cohere_api_key='sApb7nP6OfEYkQHNUpqprz5Srck5c7ZOtETachC0')

embedding_model = GoogleGenerativeAIEmbeddings(model="models/embedding-001",google_api_key='AIzaSyD_teIs1irlSn0l4ymdbCeQV89Coj7g52U')


def build_vectorstore(dataset):
    start_time = time.time()
    docs = [Document(page_content=item["context"], metadata={"question": item["question"], "answer": item["answer"]}) for item in dataset[:200]] # change this as per the excel file: See small, medium and large doc size
    end_time = time.time()  # End timing
    retrieval_time = end_time - start_time  # Compute time taken

    return Chroma.from_documents(docs, embedding_model), retrieval_time

# vectorstore_squad_small = build_vectorstore(squad_small)
vectorstore_squad_large, retrieval_time = build_vectorstore(squad_large)
print(retrieval_time)



# from langchain.chat_models import ChatOpenAI

# Load LLM (change to your custom model if needed)
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash",google_api_key='AIzaSyAct6QgpgCudTb30HF4gf_9y700mtOEfIw', temperature=0.4, convert_system_message_to_human=True)

def generate_rag_answer(question, vectorstore):
    start_time = time.time()
    retrieved_docs = vectorstore.similarity_search(question, k=10) # change as per the excel file: 1, 3, 8, 10
    context = " ".join([doc.page_content for doc in retrieved_docs])
    end_time = time.time()  # End timing
    generator_time = end_time - start_time

    response = llm.predict(f"Context: {context}\nQuestion: {question}\nAnswer:")
    return response, generator_time

from bert_score import score

def evaluate_bertscore(dataset, vectorstore):
    refs = []
    preds = []

    for item in dataset[:10]:  # Evaluate on 100 samples for efficiency
        question = item["question"]
        reference = item["answer"]

        predicted_answer,generator_time = generate_rag_answer(question, vectorstore)

        refs.append(reference)
        preds.append(predicted_answer)

    # Compute BERTScore
    P, R, F1 = score(preds, refs, lang="en", model_type="bert-base-uncased")

    return {"Precision": P.mean().item(), "Recall": R.mean().item(), "F1 Score": F1.mean().item()},generator_time

bertscore_squad_large,generator_time = evaluate_bertscore(hotpotqa_large, vectorstore_squad_large)

print("BERTScore SQuAD Large:", bertscore_squad_large)
print("Generator Time:", generator_time)